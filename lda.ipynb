{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "samples, _ = fetch_20newsgroups(\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    return_X_y=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r'\\b[a-z]+\\b')\n",
    "samples = [pattern.findall(s.lower()) for s in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopword_list = set(f.read().splitlines())\n",
    "    \n",
    "samples = [[w for w in s if w not in stopword_list] for s in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Use only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 2000\n",
    "subset = samples[:N_SAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Write a Python/NumPy Implementation of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define class for LDA with Collapsed Gibbs Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argsort, ones, zeros, zeros_like, cumsum, random, searchsorted, log\n",
    "import itertools\n",
    "\n",
    "class PythonLDA:\n",
    "    def __init__(self, corpus, T, S, beta, alpha): \n",
    "        self._init_corpus(corpus)\n",
    "        \n",
    "        self.D = D = len(self.corpus)\n",
    "        self.W = W = len(self.idx_to_word)\n",
    "        self.T = T\n",
    "        self.S = S\n",
    "        \n",
    "        self.beta_arr = beta * ones(W)\n",
    "        self.beta_sum = beta * W\n",
    "\n",
    "        self.alpha_arr = alpha * ones(T)\n",
    "        self.alpha_sum = alpha * T\n",
    "\n",
    "        self.nwt = zeros((W, T), dtype=float)\n",
    "        self.nt = zeros(T, dtype=float)\n",
    "        self.ntd = zeros((T, D), dtype=float)\n",
    "\n",
    "        self.z = [zeros(len(doc), dtype=int) for doc in corpus]\n",
    "        \n",
    "    def _init_corpus(self, corpus):\n",
    "        word_map = {}\n",
    "        for doc in corpus:\n",
    "            for word in doc:\n",
    "                if word not in word_map:\n",
    "                    word_map[word] = len(word_map)\n",
    "                word = word_map[word]\n",
    "                \n",
    "        self.corpus = [[word_map[w] for w in d] for d in corpus]\n",
    "        self.idx_to_word = {v: k for k, v in word_map.items()}\n",
    "\n",
    "    def _log_prob(self):\n",
    "        nwt = zeros_like(self.nwt)\n",
    "        nt = zeros_like(self.nt)\n",
    "        ntd = zeros_like(self.ntd)\n",
    "\n",
    "        lp = 0.0\n",
    "        for d, (doc, zd) in enumerate(zip(self.corpus, self.z)):\n",
    "            for n, (w, t) in enumerate(zip(doc, zd)):\n",
    "                first_term = (nwt[w, t] + self.beta_arr[w]) / (nt[t] + self.beta_sum)\n",
    "                second_term = (ntd[t, d] + self.alpha_arr[t]) / (n + self.alpha_sum)\n",
    "                lp += log(first_term * second_term)\n",
    "\n",
    "                nwt[w, t] += 1\n",
    "                nt[t] += 1\n",
    "                ntd[t, d] += 1\n",
    "                \n",
    "        return lp\n",
    "\n",
    "    def _sample_topics(self, init=False):\n",
    "        for d, (doc, zd) in enumerate(zip(self.corpus, self.z)):\n",
    "            for n, (w, t) in enumerate(zip(doc, zd)):\n",
    "                if not init:\n",
    "                    self.nwt[w, t] -= 1\n",
    "                    self.nt[t] -= 1\n",
    "                    self.ntd[t, d] -= 1\n",
    "\n",
    "                first_term = (self.nwt[w, :] + self.beta_arr[w]) / (self.nt + self.beta_sum)\n",
    "                second_term = (self.ntd[:, d] + self.alpha_arr)\n",
    "                dist = first_term * second_term\n",
    "\n",
    "                dist_sum = cumsum(dist)\n",
    "                r = random.random() * dist_sum[-1]\n",
    "                t = searchsorted(dist_sum, r)\n",
    "\n",
    "                self.nwt[w, t] += 1\n",
    "                self.nt[t] += 1\n",
    "                self.ntd[t, d] += 1\n",
    "\n",
    "                zd[n] = t\n",
    "\n",
    "    def fit(self):\n",
    "        self._sample_topics(init=True)\n",
    "        lp = self._log_prob()\n",
    "        print('Iteration %s: %s' % (0, lp))\n",
    "\n",
    "        for s in range(1, self.S+1):\n",
    "            self._sample_topics()\n",
    "            if not(s % (self.S//10)):\n",
    "                lp = self._log_prob()\n",
    "                print('Iteration %s: %s' % (s, lp))\n",
    "                \n",
    "        print()\n",
    "                \n",
    "                \n",
    "    def print_topics(self, num=20):\n",
    "        for t in range(self.T):\n",
    "            highest_prob_words = argsort(self.nwt[:, t] + self.beta_arr)\n",
    "            sorted_types = [self.idx_to_word[i] for i in highest_prob_words]\n",
    "            print('Topic %s: %s' % (t+1, ' '.join(sorted_types[-num:][::-1]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "py_lda = PythonLDA(corpus=subset, T=20, S=100, beta=0.01, alpha=0.1)\n",
    "cProfile.runctx('py_lda.fit()', globals(), locals(), filename=\"py_stats.txt\")\n",
    "py_lda.print_topics()\n",
    "pstats.Stats('py_stats.txt').strip_dirs().sort_stats(SortKey.TIME).print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Write a Cython Implementation of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare notebook for Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build Cython extension (Separate file because of iPython quirks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n",
      "building 'cy_lda' extension\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/joshua/repos/cython-lda/venv/include -I/usr/include/python3.8 -c cy_lda.cpp -o build/temp.linux-x86_64-3.8/cy_lda.o\n",
      "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/cy_lda.o -o /home/joshua/repos/cython-lda/cy_lda.cpython-38-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "!python3 cy_setup.py build_ext --inplace --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Repeat steps 1.1-1.4, load Cython extension, and test LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "# 1.1 Load samples\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "samples, _ = fetch_20newsgroups(\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    return_X_y=True\n",
    ")\n",
    "\n",
    "# 1.2 Tokenize samples\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'\\b[a-z]+\\b')\n",
    "samples = [pattern.findall(s.lower()) for s in samples]\n",
    "\n",
    "# 1.3 Filter stopwords\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopword_list = set(f.read().splitlines())\n",
    "samples = [[w for w in s if w not in stopword_list] for s in samples]\n",
    "\n",
    "# 1.4 Use only a subset\n",
    "N_SAMPLES = 2000\n",
    "subset = samples[:N_SAMPLES]\n",
    "\n",
    "# Test LDA\n",
    "from cy_lda import CythonLDA\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "\n",
    "cy_lda = CythonLDA(corpus=subset, T=20, S=100, beta=0.01, alpha=0.1)\n",
    "cProfile.runctx('cy_lda.fit()', globals(), locals(), filename=\"cy_stats.txt\")\n",
    "pstats.Stats('cy_stats.txt').strip_dirs().sort_stats(SortKey.TIME).print_stats(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
